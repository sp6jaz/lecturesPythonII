{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# W07 — Pandas: Czyszczenie danych\n",
    "\n",
    "**Programowanie w Pythonie II** | Politechnika Opolska\n",
    "\n",
    "Tematy:\n",
    "1. Brakujące wartości — `isna()`, `dropna()`, `fillna()`\n",
    "2. Duplikaty — `duplicated()`, `drop_duplicates()`\n",
    "3. Konwersja typów — `astype()`, `pd.to_numeric()`, `pd.to_datetime()`\n",
    "4. Operacje tekstowe — `str.lower()`, `str.strip()`, `str.replace()`, `str.contains()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "---\n",
    "## Część 1: Brakujące wartości\n",
    "\n",
    "Dane działu HR — typowy eksport z systemu ERP z wieloma problemami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brudny dataset HR\n",
    "data = {\n",
    "    'id_pracownika': [1,2,3,4,5,6,7,8,9,10,\n",
    "                      11,12,13,14,15,16,17,18,19,20,\n",
    "                      3,7,12,18,5,\n",
    "                      21,22,23,24,25],\n",
    "    'imie': ['Anna', 'Bartek', 'CELINA', 'darek', 'Ewa',\n",
    "             'Filip', 'Gosia', 'HENRYK', 'irena', 'Jan',\n",
    "             'Kasia', 'Leszek', 'Marta', 'norbert', 'OLGA',\n",
    "             'Piotr', 'Renata', 'sławek', 'Teresa', 'Urszula',\n",
    "             'CELINA', 'Gosia', 'Leszek', 'sławek', 'Ewa',\n",
    "             'Wanda', 'Xawery', 'Yvonne', 'Zbyszek', 'Agata'],\n",
    "    'dzial': ['Sprzedaz', 'IT', 'HR', 'sprzedaz', 'IT',\n",
    "              'HR', 'Sprzedaz', 'it', 'HR', 'Sprzedaz',\n",
    "              'IT', 'HR', 'sprzedaz', 'IT', 'HR',\n",
    "              'Sprzedaz', 'it', 'HR', 'Sprzedaz', 'IT',\n",
    "              'HR', 'Sprzedaz', 'HR', 'HR', 'IT',\n",
    "              'Sprzedaz', 'IT', 'hr', 'Sprzedaz', 'IT'],\n",
    "    'wynagrodzenie': ['4500', '6200', '3800', '5100', '7500',\n",
    "                      '4200', '5800', '6900', '3500', '4800',\n",
    "                      '7200', '4100', '5500', '6800', '3900',\n",
    "                      '5200', '4700', '6100', '3800', '5400',\n",
    "                      '3800', '5800', '4100', '6100', '7500',\n",
    "                      'brak', '5000', None, '4300', '6600'],\n",
    "    'data_zatrudnienia': ['2020-03-15', '2019-07-22', '2021-01-10',\n",
    "                          '2018-05-30', '2022-11-01', '2020-08-14',\n",
    "                          '2019-12-05', '2021-06-18', '2017-09-23',\n",
    "                          '2023-02-07', '2020-04-11', '2018-11-28',\n",
    "                          '2022-07-15', '2019-03-19', '2021-10-08',\n",
    "                          '2020-06-25', '2018-08-31', '2022-02-14',\n",
    "                          '2019-05-07', '2021-09-20', '2021-01-10',\n",
    "                          '2019-12-05', '2018-11-28', '2022-02-14',\n",
    "                          '2022-11-01', '2023-01-15', '2022-05-20',\n",
    "                          None, '2020-10-11', '2019-08-03'],\n",
    "    'ocena_roczna': [4.5, 3.8, None, 4.2, 5.0,\n",
    "                     3.5, 4.7, None, 4.1, 3.9,\n",
    "                     5.0, 4.3, 3.6, None, 4.8,\n",
    "                     3.7, 4.4, 4.9, None, 3.8,\n",
    "                     None, 4.7, 4.3, 4.9, 5.0,\n",
    "                     4.6, None, 4.9, 3.5, 4.1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnoza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnoza — info() pokazuje typy i liczbę non-null\n",
    "print(\"=== INFO ===\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n=== BRAKUJĄCE WARTOŚCI ===\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\n=== PROCENT BRAKÓW ===\")\n",
    "print((df.isna().sum() / len(df) * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isna_detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isna() vs isnull() — dwa identyczne warianty nazwy\n",
    "print(\"isna() == isnull():\", df.isna().equals(df.isnull()))\n",
    "\n",
    "# Wiersze z przynajmniej jednym NaN\n",
    "wiersze_z_brakiem = df[df.isna().any(axis=1)]\n",
    "print(f\"\\nWiersze z przynajmniej jednym NaN: {len(wiersze_z_brakiem)}\")\n",
    "wiersze_z_brakiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna — usuwanie wierszy z NaN\n",
    "\n",
    "# Domyślnie: usuwa wiersz jeśli MA JAKIKOLWIEK NaN\n",
    "df_dropall = df.dropna()\n",
    "print(f\"Przed dropna: {len(df)}\")\n",
    "print(f\"Po dropna (domyślnie): {len(df_dropall)}\")\n",
    "\n",
    "# subset= — usuwa tylko jeśli NaN w konkretnych kolumnach\n",
    "df_drop_data = df.dropna(subset=['data_zatrudnienia'])\n",
    "print(f\"Po dropna(subset=['data_zatrudnienia']): {len(df_drop_data)}\")\n",
    "\n",
    "# thresh= — zostaw wiersze z co najmniej N wartościami non-null\n",
    "df_thresh = df.dropna(thresh=5)\n",
    "print(f\"Po dropna(thresh=5): {len(df_thresh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fillna_strategie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna — strategie uzupełniania NaN\n",
    "\n",
    "# Strategia 1: stała wartość\n",
    "df_s1 = df.copy()\n",
    "df_s1['ocena_roczna'] = df_s1['ocena_roczna'].fillna(0)\n",
    "print(\"Strategia 1 — fillna(0):\")\n",
    "print(df_s1['ocena_roczna'].describe())\n",
    "\n",
    "# Strategia 2: średnia\n",
    "df_s2 = df.copy()\n",
    "srednia_ocena = df_s2['ocena_roczna'].mean()\n",
    "df_s2['ocena_roczna'] = df_s2['ocena_roczna'].fillna(srednia_ocena)\n",
    "print(f\"\\nStrategia 2 — fillna(mean={srednia_ocena:.2f}):\")\n",
    "print(f\"NaN po fillna: {df_s2['ocena_roczna'].isna().sum()}\")\n",
    "\n",
    "# Strategia 3: mediana (lepsza przy wartościach odstających)\n",
    "df_s3 = df.copy()\n",
    "mediana_ocena = df_s3['ocena_roczna'].median()\n",
    "df_s3['ocena_roczna'] = df_s3['ocena_roczna'].fillna(mediana_ocena)\n",
    "print(f\"\\nStrategia 3 — fillna(median={mediana_ocena:.2f}):\")\n",
    "print(f\"NaN po fillna: {df_s3['ocena_roczna'].isna().sum()}\")\n",
    "\n",
    "# Strategia 4: forward fill (przydatne w szeregach czasowych)\n",
    "df_s4 = df.copy()\n",
    "df_s4['ocena_roczna'] = df_s4['ocena_roczna'].ffill()\n",
    "print(f\"\\nStrategia 4 — ffill():\")\n",
    "print(f\"NaN po ffill: {df_s4['ocena_roczna'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "---\n",
    "## Część 2: Duplikaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicated",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated() — wykrywanie duplikatów\n",
    "\n",
    "print(f\"Liczba zduplikowanych wierszy: {df.duplicated().sum()}\")\n",
    "\n",
    "# Pokaż duplikaty (wiersze które są kopiami wcześniejszych)\n",
    "print(\"\\nZduplikowane wiersze:\")\n",
    "print(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicated_detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep=False — oznacza WSZYSTKIE wystąpienia (oryginał + kopie)\n",
    "print(\"Oryginały i duplikaty razem (keep=False):\")\n",
    "wszystkie_dup = df[df.duplicated(keep=False)].sort_values('id_pracownika')\n",
    "print(wszystkie_dup)\n",
    "\n",
    "# Duplikaty wg konkretnej kolumny\n",
    "print(f\"\\nDuplikaty wg id_pracownika: {df.duplicated(subset=['id_pracownika']).sum()}\")\n",
    "\n",
    "# Ile razy pojawia się każde id\n",
    "print(\"\\nLiczność id_pracownika (tylko te > 1):\")\n",
    "print(df['id_pracownika'].value_counts()[df['id_pracownika'].value_counts() > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drop_duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_duplicates() — usuwanie duplikatów\n",
    "\n",
    "przed = len(df)\n",
    "df_bez_dup = df.drop_duplicates()\n",
    "print(f\"Przed: {przed}, po drop_duplicates(): {len(df_bez_dup)}\")\n",
    "\n",
    "# keep='last' — zostaw ostatnie wystąpienie\n",
    "df_keep_last = df.drop_duplicates(keep='last')\n",
    "print(f\"Po drop_duplicates(keep='last'): {len(df_keep_last)}\")\n",
    "\n",
    "# Po usunięciu — reset indeksu\n",
    "df_clean = df.drop_duplicates().reset_index(drop=True)\n",
    "print(f\"\\nIndeks po reset_index: {df_clean.index.tolist()[:10]}...\")\n",
    "print(f\"Shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "---\n",
    "## Część 3: Konwersja typów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typy_diagnoza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praca na df po usunięciu duplikatów\n",
    "df_work = df.drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "print(\"Typy danych:\")\n",
    "print(df_work.dtypes)\n",
    "print()\n",
    "print(\"Wynagrodzenie — unikalne wartości:\")\n",
    "print(sorted(df_work['wynagrodzenie'].unique(), key=lambda x: str(x)))\n",
    "\n",
    "# Próba obliczenia średniej na stringu\n",
    "try:\n",
    "    print(df_work['wynagrodzenie'].mean())\n",
    "except TypeError as e:\n",
    "    print(f\"Błąd mean() na stringu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "astype_problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype() wywali błąd gdy są 'brak' lub None\n",
    "try:\n",
    "    df_work['wynagrodzenie'].astype(float)\n",
    "    print(\"astype działa!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Błąd astype(float): {e}\")\n",
    "\n",
    "# Najpierw zamieniamy 'brak' na NaN\n",
    "df_work['wynagrodzenie'] = df_work['wynagrodzenie'].replace('brak', np.nan)\n",
    "print(f\"\\nPo replace('brak' → NaN): {df_work['wynagrodzenie'].isna().sum()} braków\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to_numeric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_numeric z errors='coerce' — bezpieczna konwersja\n",
    "df_work['wynagrodzenie'] = pd.to_numeric(df_work['wynagrodzenie'], errors='coerce')\n",
    "\n",
    "print(f\"Typ po to_numeric: {df_work['wynagrodzenie'].dtype}\")\n",
    "print(f\"NaN po konwersji: {df_work['wynagrodzenie'].isna().sum()}\")\n",
    "\n",
    "# Uzupełniamy braki medianą\n",
    "mediana_wyn = df_work['wynagrodzenie'].median()\n",
    "print(f\"Mediana wynagrodzenia: {mediana_wyn}\")\n",
    "\n",
    "df_work['wynagrodzenie'] = df_work['wynagrodzenie'].fillna(mediana_wyn)\n",
    "print(f\"Po fillna(mediana): {df_work['wynagrodzenie'].isna().sum()} braków\")\n",
    "print(f\"Średnie wynagrodzenie: {df_work['wynagrodzenie'].mean():.2f} PLN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to_datetime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime() — konwersja stringów na daty\n",
    "print(\"Przed konwersją:\")\n",
    "print(df_work['data_zatrudnienia'].head(5).tolist())\n",
    "print(f\"Typ: {df_work['data_zatrudnienia'].dtype}\")\n",
    "\n",
    "df_work['data_zatrudnienia'] = pd.to_datetime(df_work['data_zatrudnienia'], errors='coerce')\n",
    "print(f\"\\nTyp po konwersji: {df_work['data_zatrudnienia'].dtype}\")\n",
    "print(df_work['data_zatrudnienia'].head(5).tolist())\n",
    "\n",
    "# .dt accessor — wydobycie elementów daty\n",
    "df_work['rok_zatrudnienia'] = df_work['data_zatrudnienia'].dt.year\n",
    "df_work['miesiac_zatrudnienia'] = df_work['data_zatrudnienia'].dt.month\n",
    "\n",
    "print(f\"\\nLata zatrudnienia: {sorted(df_work['rok_zatrudnienia'].dropna().unique().astype(int).tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "astype_category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype('category') — dla danych kategorycznych\n",
    "# Najpierw wyczyśćmy dzial (zapowiedź Materiału 4)\n",
    "df_work['dzial'] = df_work['dzial'].str.strip().str.title()\n",
    "\n",
    "print(f\"Unikalne działy: {df_work['dzial'].unique()}\")\n",
    "\n",
    "# Konwersja na typ category\n",
    "df_work['dzial_cat'] = df_work['dzial'].astype('category')\n",
    "print(f\"\\nTyp po astype('category'): {df_work['dzial_cat'].dtype}\")\n",
    "print(f\"Kategorie: {df_work['dzial_cat'].cat.categories.tolist()}\")\n",
    "\n",
    "# Porównanie pamięci\n",
    "import sys\n",
    "mem_str = df_work['dzial'].memory_usage(deep=True)\n",
    "mem_cat = df_work['dzial_cat'].memory_usage(deep=True)\n",
    "print(f\"\\nPamięć string: {mem_str} B\")\n",
    "print(f\"Pamięć category: {mem_cat} B\")\n",
    "print(f\"Oszczędność: {mem_str - mem_cat} B ({(1 - mem_cat/mem_str)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "---\n",
    "## Część 4: Operacje na tekstach (str accessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "str_podstawy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str. accessor — operacje tekstowe na całej kolumnie\n",
    "print(\"Imiona przed czyszczeniem:\")\n",
    "print(df['imie'].tolist()[:15])\n",
    "\n",
    "print(\"\\nstr.lower():\", df['imie'].str.lower().tolist()[:5])\n",
    "print(\"str.upper():\", df['imie'].str.upper().tolist()[:5])\n",
    "print(\"str.title():\", df['imie'].str.title().tolist()[:5])\n",
    "\n",
    "# str.strip() — usuwa spacje z obu końców\n",
    "test_spacje = pd.Series(['  Anna  ', 'Bartek ', ' CELINA', 'darek'])\n",
    "print(\"\\nstr.strip():\")\n",
    "print(test_spacje.str.strip().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "str_replace_contains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str.replace() i str.contains()\n",
    "\n",
    "# Normalizacja działów\n",
    "df_str = df.copy()\n",
    "print(\"Działy przed:\")\n",
    "print(df_str['dzial'].unique())\n",
    "\n",
    "# Krok 1: strip + title\n",
    "df_str['dzial'] = df_str['dzial'].str.strip().str.title()\n",
    "print(\"\\nPo title():\", df_str['dzial'].unique())\n",
    "\n",
    "# Krok 2: replace Title → wielkie litery (IT, HR)\n",
    "df_str['dzial'] = df_str['dzial'].str.replace('Hr', 'HR', regex=False)\n",
    "df_str['dzial'] = df_str['dzial'].str.replace('It', 'IT', regex=False)\n",
    "print(\"Po replace:\", df_str['dzial'].unique())\n",
    "\n",
    "# str.contains() — filtrowanie po zawartości\n",
    "it_depts = df_str[df_str['dzial'].str.contains('IT', na=False)]\n",
    "print(f\"\\nPracownicy IT: {len(it_depts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "str_kompletne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompletne czyszczenie tekstów\n",
    "df_final = df.drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "# Imiona: strip + title\n",
    "df_final['imie'] = df_final['imie'].str.strip().str.title()\n",
    "print(\"Imiona po czyszczeniu:\")\n",
    "print(df_final['imie'].tolist())\n",
    "\n",
    "# Działy: strip + title + replace\n",
    "df_final['dzial'] = df_final['dzial'].str.strip().str.title()\n",
    "df_final['dzial'] = df_final['dzial'].str.replace('Hr', 'HR', regex=False)\n",
    "df_final['dzial'] = df_final['dzial'].str.replace('It', 'IT', regex=False)\n",
    "print(f\"\\nDziały po czyszczeniu: {sorted(df_final['dzial'].unique())}\")\n",
    "print(f\"Liczba unikalnych działów: {df_final['dzial'].nunique()}\")\n",
    "print(\"\\nLiczność per dział:\")\n",
    "print(df_final['dzial'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "---\n",
    "## Pełny pipeline czyszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompletny pipeline: od brudnych danych do gotowych do analizy\n",
    "\n",
    "df_pipeline = pd.DataFrame(data)  # świeża kopia\n",
    "print(f\"START: {df_pipeline.shape}\")\n",
    "\n",
    "# Krok 1: Napraw 'brak' → NaN, konwersja wynagrodzenia\n",
    "df_pipeline['wynagrodzenie'] = df_pipeline['wynagrodzenie'].replace('brak', np.nan)\n",
    "df_pipeline['wynagrodzenie'] = pd.to_numeric(df_pipeline['wynagrodzenie'], errors='coerce')\n",
    "print(f\"Krok 1 — NaN w wynagrodzenie: {df_pipeline['wynagrodzenie'].isna().sum()}\")\n",
    "\n",
    "# Krok 2: Usuń duplikaty\n",
    "df_pipeline = df_pipeline.drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Krok 2 — shape po dedup: {df_pipeline.shape}\")\n",
    "\n",
    "# Krok 3: Wypełnij NaN w wynagrodzeniu medianą\n",
    "med = df_pipeline['wynagrodzenie'].median()\n",
    "df_pipeline['wynagrodzenie'] = df_pipeline['wynagrodzenie'].fillna(med)\n",
    "print(f\"Krok 3 — użyta mediana: {med}, NaN: {df_pipeline['wynagrodzenie'].isna().sum()}\")\n",
    "\n",
    "# Krok 4: Wypełnij NaN w ocena_roczna średnią\n",
    "avg = round(df_pipeline['ocena_roczna'].mean(), 2)\n",
    "df_pipeline['ocena_roczna'] = df_pipeline['ocena_roczna'].fillna(avg)\n",
    "print(f\"Krok 4 — użyta średnia: {avg}, NaN: {df_pipeline['ocena_roczna'].isna().sum()}\")\n",
    "\n",
    "# Krok 5: Wyczyść imię\n",
    "df_pipeline['imie'] = df_pipeline['imie'].str.strip().str.title()\n",
    "print(f\"Krok 5 — imiona: OK\")\n",
    "\n",
    "# Krok 6: Wyczyść dzial\n",
    "df_pipeline['dzial'] = df_pipeline['dzial'].str.strip().str.title()\n",
    "df_pipeline['dzial'] = df_pipeline['dzial'].str.replace('Hr', 'HR', regex=False)\n",
    "df_pipeline['dzial'] = df_pipeline['dzial'].str.replace('It', 'IT', regex=False)\n",
    "print(f\"Krok 6 — działy: {sorted(df_pipeline['dzial'].unique())}\")\n",
    "\n",
    "# Krok 7: Konwersja daty\n",
    "df_pipeline['data_zatrudnienia'] = pd.to_datetime(df_pipeline['data_zatrudnienia'], errors='coerce')\n",
    "print(f\"Krok 7 — dtype: {df_pipeline['data_zatrudnienia'].dtype}\")\n",
    "\n",
    "# Weryfikacja końcowa\n",
    "print(f\"\\n=== WERYFIKACJA ===\")\n",
    "print(f\"Shape: {df_pipeline.shape}\")\n",
    "print(f\"NaN per kolumna:\\n{df_pipeline.isna().sum()}\")\n",
    "print(f\"\\nDtypes:\\n{df_pipeline.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analiza_biznesowa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analiza biznesowa na czystych danych\n",
    "print(\"=== RAPORT HR ===\")\n",
    "print(\"\\nŚrednie wynagrodzenie per dział:\")\n",
    "print(df_pipeline.groupby('dzial')['wynagrodzenie'].mean().round(2))\n",
    "\n",
    "print(\"\\nŚrednia ocena per dział:\")\n",
    "print(df_pipeline.groupby('dzial')['ocena_roczna'].mean().round(2))\n",
    "\n",
    "print(\"\\nLiczba pracowników per dział:\")\n",
    "print(df_pipeline['dzial'].value_counts())\n",
    "\n",
    "print(\"\\nTop 3 najlepiej opłacanych:\")\n",
    "print(df_pipeline.nlargest(3, 'wynagrodzenie')[['imie', 'dzial', 'wynagrodzenie']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "podsumowanie",
   "metadata": {},
   "source": [
    "---\n",
    "## Podsumowanie\n",
    "\n",
    "| Obszar | Metody | Kiedy używać |\n",
    "|--------|--------|-------------|\n",
    "| Brakujące wartości | `isna()`, `info()` | Diagnoza |\n",
    "| Brakujące wartości | `dropna()` | Gdy wiersz bezużyteczny bez tej wartości |\n",
    "| Brakujące wartości | `fillna(mediana)` | Liczby z wartościami odstającymi |\n",
    "| Brakujące wartości | `fillna(srednia)` | Liczby z symetrycznym rozkładem |\n",
    "| Brakujące wartości | `ffill()` | Szeregi czasowe |\n",
    "| Duplikaty | `duplicated()` | Wykrycie |\n",
    "| Duplikaty | `drop_duplicates()` | Usunięcie |\n",
    "| Typy | `pd.to_numeric(errors='coerce')` | Liczby zapisane jako tekst |\n",
    "| Typy | `pd.to_datetime(errors='coerce')` | Daty zapisane jako tekst |\n",
    "| Typy | `astype('category')` | Kolumny z małą liczbą unikalnych wartości |\n",
    "| Tekst | `str.strip()`, `str.title()` | Normalizacja formatowania |\n",
    "| Tekst | `str.replace()` | Zamiana fragmentów tekstu |\n",
    "| Tekst | `str.contains()` | Filtrowanie po zawartości |\n",
    "\n",
    "**Na W08:** merge, groupby, pivot_table — analiza na czystych danych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
