{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W14 — LLM i AI w analizie danych\n",
    "## Notebook demonstracyjny — działa BEZ kluczy API\n",
    "\n",
    "**Programowanie w Pythonie II** | Wykład 14  \n",
    "**Kierunek:** Analityka danych w biznesie  \n",
    "**Politechnika Opolska**\n",
    "\n",
    "---\n",
    "\n",
    "> **UWAGA:** Ten notebook używa symulowanych odpowiedzi (mock responses) zamiast\n",
    "> prawdziwych wywołań API. Pokazuje **dokładnie takie wyniki**, jakie dostałby kod\n",
    "> z prawdziwym kluczem API. Do uruchomienia z prawdziwym API potrzeba zmiennej\n",
    "> środowiskowej `OPENAI_API_KEY` lub `ANTHROPIC_API_KEY`.\n",
    "\n",
    "### Zawartość\n",
    "1. Tokenizacja — jak LLM widzi tekst\n",
    "2. Temperatura — deterministyczny vs kreatywny\n",
    "3. Porównanie modeli GPT / Claude / Gemini\n",
    "4. Struktura wywołania API (OpenAI + Anthropic)\n",
    "5. Generowanie kodu analitycznego przez AI\n",
    "6. Interpretacja wyników statystycznych przez AI\n",
    "7. Czyszczenie danych z pomocą AI\n",
    "8. Automatyczne podsumowania (executive summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Styl wykresów\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"Środowisko gotowe — notebook działa bez kluczy API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 1: Tokenizacja — jak LLM widzi tekst\n",
    "\n",
    "LLM nie czyta liter ani słów — czyta **tokeny**. Token to fragment tekstu,\n",
    "zwykle 3-4 znaki w angielskim, więcej w polskim (fleksja tworzy więcej wariantów).\n",
    "\n",
    "**Dlaczego to ważne dla analityka:**\n",
    "- API jest rozliczane **per token** (input + output osobno)\n",
    "- Modele mają limit kontekstu wyrażony w tokenach (~100k-200k tokenów)\n",
    "- Rozumienie tokenów pomaga optymalizować koszty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 1: TOKENIZACJA (symulowana) ===\n",
    "\n",
    "def symuluj_tokenizacje(tekst, srednia_znaki_na_token=3.5):\n",
    "    \"\"\"Przybliżona symulacja tokenizacji — rzeczywista zależy od modelu.\"\"\"\n",
    "    # Prosta heurystyka: podział na słowa + znaki interpunkcyjne jako osobne tokeny\n",
    "    import re\n",
    "    czesci = re.findall(r\"[\\w']+|[^\\s\\w']\", tekst)\n",
    "    # Dłuższe słowa rozbijamy na fragmenty (przybliżenie)\n",
    "    tokeny = []\n",
    "    for czesc in czesci:\n",
    "        if len(czesc) <= 4:\n",
    "            tokeny.append(czesc)\n",
    "        elif len(czesc) <= 8:\n",
    "            tokeny.append(czesc[:4])\n",
    "            tokeny.append(czesc[4:])\n",
    "        else:\n",
    "            # Trójki\n",
    "            for i in range(0, len(czesc), 4):\n",
    "                tokeny.append(czesc[i:i+4])\n",
    "    return tokeny\n",
    "\n",
    "\n",
    "# Przykłady tekstów analitycznych\n",
    "przyklady = [\n",
    "    (\"EN\", \"Data analysis with Python\"),\n",
    "    (\"PL\", \"Analiza danych w Pythonie\"),\n",
    "    (\"EN\", \"SELECT COUNT(*) FROM orders WHERE status='cancelled'\"),\n",
    "    (\"PL\", \"Proszę przeprowadź klasteryzację klientów metodą k-średnich\"),\n",
    "    (\"PL\", \"Programowanie\"),\n",
    "    (\"EN\", \"Programming\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Język':4} | {'Tekst (skrócony)':52} | {'~Tokeny':8}\")\n",
    "print(\"-\" * 72)\n",
    "for lang, tekst in przyklady:\n",
    "    tokeny = symuluj_tokenizacje(tekst)\n",
    "    skrocony = tekst[:50] + \"...\" if len(tekst) > 50 else tekst\n",
    "    print(f\"{lang:4} | {skrocony:52} | {len(tokeny):8}\")\n",
    "\n",
    "print()\n",
    "print(\"Uwaga: To jest przybliżona symulacja. Rzeczywista tokenizacja\")\n",
    "print(\"zależy od modelu (BPE, WordPiece, SentencePiece).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WYKRES: Gęstość tokenizacji PL vs EN ===\n",
    "\n",
    "pary = [\n",
    "    (\"programowanie\",         \"programming\"),\n",
    "    (\"analiza danych\",        \"data analysis\"),\n",
    "    (\"przetwarzanie\",         \"processing\"),\n",
    "    (\"klasteryzacja\",         \"clustering\"),\n",
    "    (\"przeprowadź analizę\",   \"run analysis\"),\n",
    "    (\"uczenie maszynowe\",     \"machine learning\"),\n",
    "]\n",
    "\n",
    "etykiety = [pl for pl, en in pary]\n",
    "tokeny_pl = [len(symuluj_tokenizacje(pl)) for pl, en in pary]\n",
    "tokeny_en = [len(symuluj_tokenizacje(en)) for pl, en in pary]\n",
    "\n",
    "x = np.arange(len(etykiety))\n",
    "szerokosc = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "belki_pl = ax.bar(x - szerokosc/2, tokeny_pl, szerokosc, label='Polski', color='#e74c3c', alpha=0.8)\n",
    "belki_en = ax.bar(x + szerokosc/2, tokeny_en, szerokosc, label='Angielski', color='#3498db', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(etykiety, rotation=15, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Przybliżona liczba tokenów')\n",
    "ax.set_title('Tokenizacja: Polski vs Angielski\\n(języki fleksyjne = więcej tokenów = wyższy koszt API)',\n",
    "             fontsize=11)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(max(tokeny_pl), max(tokeny_en)) + 2)\n",
    "\n",
    "for belka in belki_pl:\n",
    "    ax.text(belka.get_x() + belka.get_width()/2., belka.get_height() + 0.1,\n",
    "            f'{int(belka.get_height())}', ha='center', va='bottom', fontsize=9)\n",
    "for belka in belki_en:\n",
    "    ax.text(belka.get_x() + belka.get_width()/2., belka.get_height() + 0.1,\n",
    "            f'{int(belka.get_height())}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 2: Temperatura — deterministyczny vs kreatywny\n",
    "\n",
    "Temperatura kontroluje \"losowość\" przy wyborze następnego tokenu.\n",
    "\n",
    "| Temperatura | Zastosowanie | Przykład |\n",
    "|-------------|-------------|----------|\n",
    "| 0.0 | Kod, SQL, obliczenia | Generowanie funkcji Python |\n",
    "| 0.3–0.7 | Wyjaśnienia, raporty | Interpretacja wyników |\n",
    "| 1.0–1.5 | Kreatywność | Brainstorming, nazwy kampanii |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 2: SYMULACJA EFEKTU TEMPERATURY ===\n",
    "\n",
    "# Symulujemy: jak różna temperatura wpływa na odpowiedzi\n",
    "# (prawdziwe modele są bardziej złożone, ale koncepcja jest oddana)\n",
    "\n",
    "pytanie = \"Jak nazywa się metoda analizy skupień w uczeniu maszynowym?\"\n",
    "\n",
    "odpowiedzi = {\n",
    "    0.0: [\n",
    "        \"Klasteryzacja (ang. clustering).\",\n",
    "        \"Klasteryzacja (ang. clustering).\",\n",
    "        \"Klasteryzacja (ang. clustering).\",\n",
    "    ],\n",
    "    0.7: [\n",
    "        \"Klasteryzacja (clustering). Popularne algorytmy to K-Means i DBSCAN.\",\n",
    "        \"Analiza skupień (clustering) — grupowanie podobnych obserwacji. Np. K-Means.\",\n",
    "        \"Klasteryzacja. W Pythonie: sklearn.cluster.KMeans lub AgglomerativeClustering.\",\n",
    "    ],\n",
    "    1.5: [\n",
    "        \"To magia grupowania danych — klasteryzacja! Niczym sortowanie klocków Lego.\",\n",
    "        \"Mówię o klasteryzacji — ale możesz to też nazwać 'sztuką znajdowania rodzin w danych'.\",\n",
    "        \"Analiza skupień, klasteryzacja, segment analysis — wiele nazw dla jednej idei!\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(f\"Pytanie: {pytanie}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for temp, odp_lista in odpowiedzi.items():\n",
    "    opis = {0.0: \"KOD/FAKTY\", 0.7: \"ZRÓWNOWAŻONA\", 1.5: \"KREATYWNA\"}\n",
    "    print(f\"\\nTemperatura = {temp} ({opis[temp]}):\")\n",
    "    for i, odp in enumerate(odp_lista, 1):\n",
    "        print(f\"  Próba {i}: {odp}\")\n",
    "\n",
    "print()\n",
    "print(\"WNIOSEK: Przy temp=0.0 każda próba daje IDENTYCZNĄ odpowiedź.\")\n",
    "print(\"         Przy temp=1.5 każda próba jest inna — kreatywna, ale mniej przewidywalna.\")\n",
    "print(\"         W analizie danych: ZAWSZE temperatura blisko 0 dla kodu i obliczeń.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 3: Porównanie modeli — GPT vs Claude vs Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 3: PORÓWNANIE MODELI ===\n",
    "\n",
    "# Dane porównawcze (orientacyjne, stan na 2025)\n",
    "modele = [\n",
    "    {\n",
    "        \"nazwa\": \"GPT-4o\",\n",
    "        \"firma\": \"OpenAI\",\n",
    "        \"kontekst_k_tokenow\": 128,\n",
    "        \"mocne_strony\": [\"Kod i matematyka\", \"Integracja Azure/MS\", \"Function calling\"],\n",
    "        \"slabsze_strony\": [\"Droższy\", \"Halucynacje przy faktach\"],\n",
    "        \"cena_input_per_1m\": 2.50,\n",
    "        \"cena_output_per_1m\": 10.00,\n",
    "        \"kolor\": \"#10a37f\",\n",
    "    },\n",
    "    {\n",
    "        \"nazwa\": \"Claude Sonnet\",\n",
    "        \"firma\": \"Anthropic\",\n",
    "        \"kontekst_k_tokenow\": 200,\n",
    "        \"mocne_strony\": [\"Długie dokumenty\", \"Śledzenie instrukcji\", \"Mniej halucynacji\"],\n",
    "        \"slabsze_strony\": [\"Mniej integracji\", \"Bywa ostrożny\"],\n",
    "        \"cena_input_per_1m\": 3.00,\n",
    "        \"cena_output_per_1m\": 15.00,\n",
    "        \"kolor\": \"#cc785c\",\n",
    "    },\n",
    "    {\n",
    "        \"nazwa\": \"Gemini 1.5\",\n",
    "        \"firma\": \"Google\",\n",
    "        \"kontekst_k_tokenow\": 1000,\n",
    "        \"mocne_strony\": [\"Google Workspace\", \"Multimodalność\", \"BigQuery integracja\"],\n",
    "        \"slabsze_strony\": [\"Mniej dojrzały API\", \"Mniejsza społeczność\"],\n",
    "        \"cena_input_per_1m\": 1.25,\n",
    "        \"cena_output_per_1m\": 5.00,\n",
    "        \"kolor\": \"#4285f4\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"{'Model':15} {'Firma':12} {'Kontekst':12} {'Input $/1M':12} {'Output $/1M':12}\")\n",
    "print(\"-\" * 65)\n",
    "for m in modele:\n",
    "    print(f\"{m['nazwa']:15} {m['firma']:12} {m['kontekst_k_tokenow']:>6} k tok   \"\n",
    "          f\"${m['cena_input_per_1m']:>6.2f}       ${m['cena_output_per_1m']:>6.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"Mocne strony:\")\n",
    "for m in modele:\n",
    "    print(f\"  {m['nazwa']:15}: {', '.join(m['mocne_strony'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WYKRES: Kontekst i koszty modeli ===\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "nazwy = [m['nazwa'] for m in modele]\n",
    "kolory = [m['kolor'] for m in modele]\n",
    "konteksty = [m['kontekst_k_tokenow'] for m in modele]\n",
    "ceny_input = [m['cena_input_per_1m'] for m in modele]\n",
    "ceny_output = [m['cena_output_per_1m'] for m in modele]\n",
    "\n",
    "# Panel 1: Kontekst\n",
    "bars1 = axes[0].barh(nazwy, konteksty, color=kolory, alpha=0.85, edgecolor='white')\n",
    "axes[0].set_xlabel('Okno kontekstu (tysiące tokenów)')\n",
    "axes[0].set_title('Okno kontekstu\\n(im większe, tym dłuższy dokument model widzi naraz)')\n",
    "for bar, val in zip(bars1, konteksty):\n",
    "    axes[0].text(val + 10, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{val}k', va='center', fontsize=10)\n",
    "axes[0].set_xlim(0, max(konteksty) * 1.2)\n",
    "\n",
    "# Panel 2: Koszty\n",
    "x = np.arange(len(nazwy))\n",
    "szer = 0.35\n",
    "axes[1].bar(x - szer/2, ceny_input, szer, label='Input ($/1M tok)', color=kolory, alpha=0.6)\n",
    "axes[1].bar(x + szer/2, ceny_output, szer, label='Output ($/1M tok)', color=kolory, alpha=0.9)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(nazwy)\n",
    "axes[1].set_ylabel('Cena (USD / milion tokenów)')\n",
    "axes[1].set_title('Porównanie kosztów API\\n(orientacyjne ceny, stan 2025)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 4: Struktura wywołania API\n",
    "\n",
    "Kod poniżej pokazuje **jak wygląda wywołanie API** — komentarze wskazują co zmienić\n",
    "żeby użyć prawdziwego klucza zamiast mock odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 4A: MOCK API — OpenAI struktura ===\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WERSJA PRODUKCYJNA (wymaga klucza API):\n",
    "#\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()  # czyta OPENAI_API_KEY z zmiennych środowiskowych\n",
    "#\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4o\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_msg},\n",
    "#         {\"role\": \"user\",   \"content\": user_msg}\n",
    "#     ],\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=500\n",
    "# )\n",
    "# odpowiedz = response.choices[0].message.content\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# WERSJA MOCK (działa bez klucza — do celów dydaktycznych):\n",
    "\n",
    "class MockOpenAIResponse:\n",
    "    \"\"\"Symuluje strukturę odpowiedzi OpenAI API.\"\"\"\n",
    "    def __init__(self, content, model=\"gpt-4o\", input_tokens=150, output_tokens=80):\n",
    "        self.choices = [type('Choice', (), {\n",
    "            'message': type('Message', (), {'content': content, 'role': 'assistant'})()\n",
    "        })()]\n",
    "        self.model = model\n",
    "        self.usage = type('Usage', (), {\n",
    "            'prompt_tokens': input_tokens,\n",
    "            'completion_tokens': output_tokens,\n",
    "            'total_tokens': input_tokens + output_tokens\n",
    "        })()\n",
    "\n",
    "def mock_openai_call(system_msg, user_msg, model=\"gpt-4o\",\n",
    "                      temperature=0.1, max_tokens=500):\n",
    "    \"\"\"Mock wywołania OpenAI — zwraca strukturę identyczną z prawdziwym API.\"\"\"\n",
    "    # W produkcji: zamień na prawdziwe wywołanie (patrz komentarz powyżej)\n",
    "    from_tokens = len(system_msg.split()) + len(user_msg.split())\n",
    "\n",
    "    # Predefiniowane mock odpowiedzi\n",
    "    mock_answers = {\n",
    "        \"p-wartosc\": \"\"\"P-wartość (p-value) to prawdopodobieństwo uzyskania\n",
    "wyniku co najmniej tak ekstremalnego jak zaobserwowany, przy założeniu\n",
    "że hipoteza zerowa jest prawdziwa.\n",
    "\n",
    "Prościej: jeśli p < 0.05, różnica którą widzimy jest zbyt duża\n",
    "żeby być przypadkiem — odrzucamy hipotezę zerową.\"\"\",\n",
    "        \"default\": \"Odpowiedź modelu AI na zadane pytanie analityczne.\"\n",
    "    }\n",
    "\n",
    "    klucz = \"p-wartosc\" if \"p-wartość\" in user_msg.lower() or \"p-value\" in user_msg.lower() else \"default\"\n",
    "    tresc = mock_answers[klucz]\n",
    "\n",
    "    return MockOpenAIResponse(\n",
    "        content=tresc,\n",
    "        model=model,\n",
    "        input_tokens=from_tokens,\n",
    "        output_tokens=len(tresc.split())\n",
    "    )\n",
    "\n",
    "\n",
    "# === Przykład użycia ===\n",
    "system_msg = \"Jesteś asystentem analityka danych. Odpowiadaj po polsku, zwięźle.\"\n",
    "user_msg   = \"Wyjaśnij czym jest p-wartość w jednym akapicie.\"\n",
    "\n",
    "response = mock_openai_call(system_msg, user_msg)\n",
    "\n",
    "print(\"=== Wywołanie API (mock) ===\")\n",
    "print(f\"Model:       {response.model}\")\n",
    "print(f\"Tokeny:      {response.usage.prompt_tokens} input + {response.usage.completion_tokens} output\")\n",
    "print(f\"Koszt (~):   ${response.usage.total_tokens / 1_000_000 * 2.50:.6f} USD\")\n",
    "print()\n",
    "print(\"Odpowiedź modelu:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 4B: MOCK API — Anthropic struktura ===\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WERSJA PRODUKCYJNA (wymaga klucza API):\n",
    "#\n",
    "# import anthropic\n",
    "# client = anthropic.Anthropic()  # czyta ANTHROPIC_API_KEY\n",
    "#\n",
    "# message = client.messages.create(\n",
    "#     model=\"claude-sonnet-4-5\",\n",
    "#     max_tokens=500,\n",
    "#     system=system_msg,\n",
    "#     messages=[{\"role\": \"user\", \"content\": user_msg}]\n",
    "# )\n",
    "# odpowiedz = message.content[0].text\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class MockAnthropicResponse:\n",
    "    \"\"\"Symuluje strukturę odpowiedzi Anthropic API.\"\"\"\n",
    "    def __init__(self, content, model=\"claude-sonnet-4-5\"):\n",
    "        self.content = [type('Block', (), {'text': content, 'type': 'text'})()]\n",
    "        self.model = model\n",
    "        self.stop_reason = \"end_turn\"\n",
    "\n",
    "\n",
    "def mock_anthropic_call(system_msg, user_msg, model=\"claude-sonnet-4-5\", max_tokens=500):\n",
    "    \"\"\"Mock wywołania Anthropic — identyczna struktura z prawdziwym API.\"\"\"\n",
    "    tresc = \"\"\"Korelacja r=0.15 jest **słaba** (prawie brak związku liniowego).\n",
    "\n",
    "Według skali Cohena:\n",
    "- r < 0.1 → zaniedbywalna\n",
    "- 0.1–0.3 → słaba\n",
    "- 0.3–0.5 → umiarkowana\n",
    "- r > 0.5 → silna\n",
    "\n",
    "r=0.15 mieści się w przedziale słabej korelacji.\n",
    "Oznacza to, że wiek wyjaśnia tylko r² = 2.25% wariancji satysfakcji — \n",
    "inne czynniki mają o wiele większe znaczenie.\"\"\"\n",
    "    return MockAnthropicResponse(content=tresc, model=model)\n",
    "\n",
    "\n",
    "# Demonstracja\n",
    "system = \"Jesteś ekspertem statystyki. Odpowiadaj zwięźle, po polsku.\"\n",
    "user   = \"Czy korelacja r=0.15 między wiekiem a satysfakcją z pracy jest silna?\"\n",
    "\n",
    "msg = mock_anthropic_call(system, user)\n",
    "\n",
    "print(\"=== Wywołanie Anthropic API (mock) ===\")\n",
    "print(f\"Model: {msg.model} | Stop reason: {msg.stop_reason}\")\n",
    "print()\n",
    "print(\"Odpowiedź Claude:\")\n",
    "print(\"-\" * 50)\n",
    "print(msg.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 4C: STRUCTURED OUTPUT ===\n",
    "\n",
    "def mock_structured_call(pytanie, dane_kontekst=\"\"):\n",
    "    \"\"\"\n",
    "    Symuluje wywołanie z prośbą o odpowiedź w JSON.\n",
    "    W produkcji: użyj response_format={\"type\": \"json_object\"} w OpenAI\n",
    "    lub odpowiednio sformułowanego system message.\n",
    "    \"\"\"\n",
    "    # Mock JSON responses per typ pytania\n",
    "    if \"korelacja\" in pytanie.lower() and \"0.15\" in pytanie:\n",
    "        return {\n",
    "            \"wynik\": \"Korelacja r=0.15 jest bardzo słaba — wiek praktycznie nie wyjaśnia satysfakcji z pracy.\",\n",
    "            \"pewnosc\": 95,\n",
    "            \"ostrzezenie\": \"Korelacja liniowa nie wyklucza związku nieliniowego (U-kształt) — warto sprawdzić scatterplot.\"\n",
    "        }\n",
    "    elif \"p-warto\" in pytanie.lower() and (\"0.03\" in pytanie or \"0.031\" in pytanie):\n",
    "        return {\n",
    "            \"wynik\": \"p=0.031 < 0.05 — wynik statystycznie istotny, odrzucamy hipotezę zerową.\",\n",
    "            \"pewnosc\": 99,\n",
    "            \"ostrzezenie\": None\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"wynik\": \"Odpowiedź na zadane pytanie analityczne.\",\n",
    "            \"pewnosc\": 80,\n",
    "            \"ostrzezenie\": \"Pytanie zbyt ogólne — podaj więcej kontekstu.\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Test 1: pytanie o korelację\n",
    "odpowiedz_json = mock_structured_call(\n",
    "    \"Czy korelacja r=0.15 między wiekiem a satysfakcją z pracy jest silna?\"\n",
    ")\n",
    "\n",
    "print(\"=== Structured Output (JSON) ===\")\n",
    "print(json.dumps(odpowiedz_json, ensure_ascii=False, indent=2))\n",
    "print()\n",
    "\n",
    "# Programowe użycie wyników\n",
    "print(f\"Wynik:      {odpowiedz_json['wynik']}\")\n",
    "print(f\"Pewność:    {odpowiedz_json['pewnosc']}%\")\n",
    "if odpowiedz_json.get('ostrzezenie'):\n",
    "    print(f\"Ostrzeżenie: {odpowiedz_json['ostrzezenie']}\")\n",
    "\n",
    "print()\n",
    "print(\"Zaleta structured output: JSON gotowy do dalszego przetwarzania\")\n",
    "print(\"— nie trzeba parsować tekstu, pola są przewidywalne.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 5: Generowanie kodu analitycznego przez AI\n",
    "\n",
    "Technika 1: Precyzyjny prompt z kontekstem → kod gotowy do użycia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 5: GENEROWANIE KODU — DATASET I DEMONSTRACJA ===\n",
    "\n",
    "# Tworzymy dataset który będzie używany w kolejnych sekcjach\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "df_sprzedaz = pd.DataFrame({\n",
    "    'data_zamowienia': pd.date_range('2024-01-01', periods=n, freq='D')[:n],\n",
    "    'klient_id': np.random.randint(1, 101, n),\n",
    "    'produkt_kategoria': np.random.choice(\n",
    "        ['elektronika', 'odzież', 'dom_i_ogrod', 'sport', 'ksiazki'],\n",
    "        n, p=[0.30, 0.25, 0.20, 0.15, 0.10]\n",
    "    ),\n",
    "    'wartosc_zamowienia': np.round(np.random.lognormal(5.5, 0.8, n), 2),\n",
    "    'status': np.random.choice(\n",
    "        ['zrealizowane', 'anulowane', 'zwrócone'],\n",
    "        n, p=[0.78, 0.12, 0.10]\n",
    "    ),\n",
    "})\n",
    "df_sprzedaz['data_zamowienia'] = pd.to_datetime(df_sprzedaz['data_zamowienia'])\n",
    "\n",
    "print(\"Dataset do demonstracji:\")\n",
    "print(df_sprzedaz.head())\n",
    "print(f\"\\nKształt: {df_sprzedaz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEMONSTRACJA: Prompt → wygenerowany kod (symulacja) ===\n",
    "\n",
    "dobry_prompt = \"\"\"\n",
    "Mam DataFrame pandas `df_sprzedaz` z kolumnami:\n",
    "- data_zamowienia (datetime)\n",
    "- klient_id (int)\n",
    "- produkt_kategoria (str: 'elektronika', 'odzież', 'dom_i_ogrod', 'sport', 'ksiazki')\n",
    "- wartosc_zamowienia (float, PLN)\n",
    "- status (str: 'zrealizowane', 'anulowane', 'zwrócone')\n",
    "\n",
    "Zadanie: Napisz funkcję `raport_kategorii(df)` która:\n",
    "1. Filtruje tylko status='zrealizowane'\n",
    "2. Grupuje per kategoria: suma przychodu, liczba zamówień, średnia wartość\n",
    "3. Dodaje kolumnę 'udzial_pct' (% przychodu tej kategorii)\n",
    "4. Sortuje malejąco po przychodzie\n",
    "5. Zwraca DataFrame\n",
    "\n",
    "Ograniczenia: tylko pandas i numpy.\n",
    "\"\"\"\n",
    "\n",
    "# Symulowany wygenerowany kod (dokładnie taki jaki dałoby API):\n",
    "wygenerowany_kod = '''\n",
    "def raport_kategorii(df):\n",
    "    \"\"\"Raport sprzedaży per kategoria — tylko zamówienia zrealizowane.\"\"\"\n",
    "    df_real = df[df['status'] == 'zrealizowane'].copy()\n",
    "\n",
    "    raport = (\n",
    "        df_real.groupby('produkt_kategoria')['wartosc_zamowienia']\n",
    "        .agg(\n",
    "            przychod='sum',\n",
    "            zamowienia='count',\n",
    "            sr_wartosc='mean'\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    raport['przychod'] = raport['przychod'].round(2)\n",
    "    raport['sr_wartosc'] = raport['sr_wartosc'].round(2)\n",
    "    raport['udzial_pct'] = (raport['przychod'] / raport['przychod'].sum() * 100).round(1)\n",
    "\n",
    "    return raport.sort_values('przychod', ascending=False).reset_index(drop=True)\n",
    "'''\n",
    "\n",
    "print(\"=== PROMPT (co wysyłamy do AI) ===\")\n",
    "print(dobry_prompt)\n",
    "print(\"=\" * 55)\n",
    "print(\"\\n=== KOD WYGENEROWANY PRZEZ AI ===\")\n",
    "print(wygenerowany_kod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === URUCHOMIENIE WYGENEROWANEGO KODU ===\n",
    "# (dokładnie tak samo jak w prawdziwym workflow — wklejamy i uruchamiamy)\n",
    "\n",
    "def raport_kategorii(df):\n",
    "    \"\"\"Raport sprzedaży per kategoria — tylko zamówienia zrealizowane.\"\"\"\n",
    "    df_real = df[df['status'] == 'zrealizowane'].copy()\n",
    "\n",
    "    raport = (\n",
    "        df_real.groupby('produkt_kategoria')['wartosc_zamowienia']\n",
    "        .agg(\n",
    "            przychod='sum',\n",
    "            zamowienia='count',\n",
    "            sr_wartosc='mean'\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    raport['przychod'] = raport['przychod'].round(2)\n",
    "    raport['sr_wartosc'] = raport['sr_wartosc'].round(2)\n",
    "    raport['udzial_pct'] = (raport['przychod'] / raport['przychod'].sum() * 100).round(1)\n",
    "\n",
    "    return raport.sort_values('przychod', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Uruchamiamy — weryfikacja czy kod działa\n",
    "wynik = raport_kategorii(df_sprzedaz)\n",
    "print(\"Raport per kategoria (wygenerowany przez AI, uruchomiony i zweryfikowany):\")\n",
    "print(wynik.to_string(index=False))\n",
    "print(f\"\\nKontrola: suma udziałów = {wynik['udzial_pct'].sum():.1f}% (powinno być ~100%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WIZUALIZACJA WYNIKÓW Z RAPORTU ===\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "kolory = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6']\n",
    "\n",
    "# Panel 1: Przychód per kategoria\n",
    "axes[0].barh(\n",
    "    wynik['produkt_kategoria'],\n",
    "    wynik['przychod'],\n",
    "    color=kolory[:len(wynik)],\n",
    "    alpha=0.85,\n",
    "    edgecolor='white'\n",
    ")\n",
    "axes[0].set_xlabel('Przychód (PLN)')\n",
    "axes[0].set_title('Przychód per kategoria\\n(tylko zamówienia zrealizowane)', fontsize=11)\n",
    "axes[0].invert_yaxis()\n",
    "for i, (v, u) in enumerate(zip(wynik['przychod'], wynik['udzial_pct'])):\n",
    "    axes[0].text(v + 1000, i, f'{v:,.0f} PLN ({u}%)', va='center', fontsize=9)\n",
    "axes[0].set_xlim(0, wynik['przychod'].max() * 1.35)\n",
    "\n",
    "# Panel 2: Liczba zamówień\n",
    "axes[1].bar(\n",
    "    wynik['produkt_kategoria'],\n",
    "    wynik['zamowienia'],\n",
    "    color=kolory[:len(wynik)],\n",
    "    alpha=0.85,\n",
    "    edgecolor='white'\n",
    ")\n",
    "axes[1].set_ylabel('Liczba zamówień')\n",
    "axes[1].set_title('Liczba zamówień per kategoria', fontsize=11)\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('Analiza sprzedaży — raport kategorii\\n(kod wygenerowany przez AI, zweryfikowany)',\n",
    "             fontsize=12, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 6: Interpretacja wyników statystycznych przez AI\n",
    "\n",
    "Technika 2: Wyniki analizy + prompt → tekst dla decydenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 6: INTERPRETACJA WYNIKÓW A/B TESTU ===\n",
    "\n",
    "# Symulowane wyniki A/B testu (jak z W12)\n",
    "np.random.seed(42)\n",
    "koszyk_A = np.random.normal(loc=320, scale=75, size=150)\n",
    "koszyk_B = np.random.normal(loc=358, scale=80, size=150)\n",
    "\n",
    "# Obliczenia (prawdziwe — bez AI)\n",
    "srednia_A = koszyk_A.mean()\n",
    "srednia_B = koszyk_B.mean()\n",
    "roznica = srednia_B - srednia_A\n",
    "pct_wzrost = roznica / srednia_A * 100\n",
    "\n",
    "# Uproszczony t-test ręcznie (scipy nie jest wymagane w tej sekcji)\n",
    "n_A, n_B = len(koszyk_A), len(koszyk_B)\n",
    "se_diff = np.sqrt(koszyk_A.var() / n_A + koszyk_B.var() / n_B)\n",
    "t_stat = roznica / se_diff\n",
    "# p-wartość przybliżona (dla dużych prób rozkład t ≈ normalny)\n",
    "from scipy import stats as scipy_stats\n",
    "p_val = 2 * (1 - scipy_stats.norm.cdf(abs(t_stat)))\n",
    "\n",
    "# 95% CI dla różnicy\n",
    "z_95 = 1.96\n",
    "ci_low = roznica - z_95 * se_diff\n",
    "ci_high = roznica + z_95 * se_diff\n",
    "\n",
    "print(\"=== Wyniki A/B testu kampanii e-mailowej ===\")\n",
    "print(f\"Wersja A (n={n_A}): x̄ = {srednia_A:.2f} PLN\")\n",
    "print(f\"Wersja B (n={n_B}): x̄ = {srednia_B:.2f} PLN\")\n",
    "print(f\"Różnica:             {roznica:+.2f} PLN ({pct_wzrost:+.1f}%)\")\n",
    "print(f\"t-stat:              {t_stat:.4f}\")\n",
    "print(f\"p-wartość:           {p_val:.6f}\")\n",
    "print(f\"95% CI:              [{ci_low:.2f}, {ci_high:.2f}] PLN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MOCK AI: INTERPRETACJA WYNIKÓW ===\n",
    "\n",
    "wyniki_do_interpretacji = f\"\"\"\n",
    "A/B test kampanii e-mailowej:\n",
    "- Wersja A (n={n_A}): średnia wartość koszyka = {srednia_A:.2f} PLN\n",
    "- Wersja B (n={n_B}): średnia wartość koszyka = {srednia_B:.2f} PLN\n",
    "- Welch's t-test: t = {t_stat:.2f}, p = {p_val:.6f}\n",
    "- 95% CI dla różnicy (B-A): [{ci_low:.1f}, {ci_high:.1f}] PLN\n",
    "\"\"\"\n",
    "\n",
    "# Mock odpowiedź — dokładnie jaka przyszłaby z prawdziwego API\n",
    "def mock_interpretuj_ab_test(wyniki: str, klientow_miesiecznie: int = 50000) -> str:\n",
    "    \"\"\"Symuluje interpretację AI wyników A/B testu.\"\"\"\n",
    "    szac_wzrost_miesiecznie = roznica * klientow_miesiecznie\n",
    "    szac_wzrost_rocznie = szac_wzrost_miesiecznie * 12\n",
    "\n",
    "    return f\"\"\"**Raport: A/B test kampanii e-mail**\n",
    "\n",
    "**Główny wniosek:** Personalizowany email (wersja B) statystycznie istotnie\n",
    "zwiększa wartość koszyka o ok. {roznica:.0f} PLN na transakcję.\n",
    "\n",
    "**Co to znaczy w praktyce:** Klienci którzy otrzymali spersonalizowaną wiadomość\n",
    "kupowali średnio o {roznica:.0f} PLN więcej (+{pct_wzrost:.1f}%). Wynik jest\n",
    "statystycznie istotny (p < 0.001) — szansa przypadku jest mniejsza niż 0.1%.\n",
    "Nawet w pesymistycznym scenariuszu (dolna granica CI = {ci_low:.0f} PLN)\n",
    "personalizacja przynosi pozytywny efekt.\n",
    "\n",
    "**Szacunek finansowy:** Przy {klientow_miesiecznie:,} emailach miesięcznie:\n",
    "- Dodatkowy przychód miesięcznie: ~{szac_wzrost_miesiecznie:,.0f} PLN\n",
    "- Dodatkowy przychód rocznie:     ~{szac_wzrost_rocznie:,.0f} PLN\n",
    "\n",
    "**Rekomendacja:** Wdrożyć wersję B jako standard kampanii e-mailowych.\n",
    "\n",
    "**Zastrzeżenie:** Wyniki dotyczą jednej kampanii. Zalecamy powtórzenie\n",
    "testu w innym okresie sezonowym przed pełnym wdrożeniem.\"\"\"\n",
    "\n",
    "\n",
    "interpretacja = mock_interpretuj_ab_test(wyniki_do_interpretacji)\n",
    "\n",
    "print(\"=== INTERPRETACJA AI (mock — identyczna z prawdziwą odpowiedzią modelu) ===\")\n",
    "print(interpretacja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WIZUALIZACJA A/B TESTU ===\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Histogramy\n",
    "axes[0].hist(koszyk_A, bins=25, alpha=0.6, color='#3498db', density=True,\n",
    "             label=f'Wersja A\\n(x̄={srednia_A:.0f} PLN)')\n",
    "axes[0].hist(koszyk_B, bins=25, alpha=0.6, color='#e74c3c', density=True,\n",
    "             label=f'Wersja B\\n(x̄={srednia_B:.0f} PLN)')\n",
    "axes[0].axvline(srednia_A, color='#3498db', linestyle='--', lw=1.8)\n",
    "axes[0].axvline(srednia_B, color='#e74c3c', linestyle='--', lw=1.8)\n",
    "axes[0].set_title(f'Rozkład wartości koszyka\\np={p_val:.4f}', fontsize=10)\n",
    "axes[0].set_xlabel('Wartość koszyka (PLN)')\n",
    "axes[0].set_ylabel('Gęstość')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Panel 2: Boxplot\n",
    "bp = axes[1].boxplot(\n",
    "    [koszyk_A, koszyk_B],\n",
    "    labels=['Wersja A', 'Wersja B'],\n",
    "    patch_artist=True,\n",
    "    medianprops=dict(color='black', linewidth=2)\n",
    ")\n",
    "bp['boxes'][0].set_facecolor('#3498db')\n",
    "bp['boxes'][0].set_alpha(0.6)\n",
    "bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "bp['boxes'][1].set_alpha(0.6)\n",
    "axes[1].set_title('Boxplot porównawczy', fontsize=10)\n",
    "axes[1].set_ylabel('Wartość koszyka (PLN)')\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 3: Przedział ufności\n",
    "axes[2].errorbar(\n",
    "    x=['Różnica B–A'],\n",
    "    y=[roznica],\n",
    "    yerr=[[roznica - ci_low], [ci_high - roznica]],\n",
    "    fmt='o', color='#27ae60', markersize=12,\n",
    "    capsize=15, linewidth=2.5,\n",
    "    label=f'+{roznica:.1f} PLN'\n",
    ")\n",
    "axes[2].axhline(0, color='red', linestyle='--', lw=1.5, label='H₀: brak różnicy')\n",
    "axes[2].set_title(f'95% CI dla różnicy\\n(cały przedział > 0 → wdrożyć B)', fontsize=10)\n",
    "axes[2].set_ylabel('Różnica (PLN)')\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('A/B Test kampanii e-mailowej — wyniki + interpretacja AI',\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 7: Czyszczenie danych z pomocą AI\n",
    "\n",
    "Technika 3: AI generuje słownik mapowania dla niejednorodnych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 7: CZYSZCZENIE OPISÓW ===\n",
    "\n",
    "# Realistyczny problem: kolumna statusów wpisywana ręcznie przez pracowników\n",
    "np.random.seed(42)\n",
    "\n",
    "# 200 rekordów z niejednorodnie wpisanymi statusami\n",
    "warianty_raw = [\n",
    "    \"zrealizowane\", \"Zrealizowane\", \"ZREALIZOWANE\", \"zrealizowanoe\", \"zrealiz.\",\n",
    "    \"anulowane\", \"Anulowane\", \"ANULOWANE\", \"anulowanie\", \"anulow.\",\n",
    "    \"zwrot\", \"Zwrot\", \"ZWROT\", \"zwrócone\", \"zwrocone\", \"zwrót\",\n",
    "    \"w realizacji\", \"W Realizacji\", \"W trakcie realizacji\", \"w_realizacji\",\n",
    "    \"oczekuje\", \"oczekuje na płatność\", \"Oczekuje na płatnosc\", \"do zapłaty\",\n",
    "]\n",
    "\n",
    "kolumna_statusow = pd.Series(\n",
    "    np.random.choice(warianty_raw, size=200)\n",
    ")\n",
    "\n",
    "print(\"Unikalne wartości w surowej kolumnie statusów:\")\n",
    "print(f\"Liczba unikalnych: {kolumna_statusow.nunique()}\")\n",
    "print(kolumna_statusow.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MOCK AI: NORMALIZACJA KATEGORII ===\n",
    "\n",
    "# To jest odpowiedź AI na prompt:\n",
    "# \"Napisz słownik Python mapujący warianty statusów na 5 docelowych kategorii\"\n",
    "\n",
    "MAPA_STATUSOW = {\n",
    "    # Zrealizowane\n",
    "    \"zrealizowane\": \"zrealizowane\",\n",
    "    \"Zrealizowane\": \"zrealizowane\",\n",
    "    \"ZREALIZOWANE\": \"zrealizowane\",\n",
    "    \"zrealizowanoe\": \"zrealizowane\",    # literówka\n",
    "    \"zrealiz.\": \"zrealizowane\",\n",
    "    # Anulowane\n",
    "    \"anulowane\": \"anulowane\",\n",
    "    \"Anulowane\": \"anulowane\",\n",
    "    \"ANULOWANE\": \"anulowane\",\n",
    "    \"anulowanie\": \"anulowane\",\n",
    "    \"anulow.\": \"anulowane\",\n",
    "    # Zwrócone\n",
    "    \"zwrot\": \"zwrócone\",\n",
    "    \"Zwrot\": \"zwrócone\",\n",
    "    \"ZWROT\": \"zwrócone\",\n",
    "    \"zwrócone\": \"zwrócone\",\n",
    "    \"zwrocone\": \"zwrócone\",\n",
    "    \"zwrót\": \"zwrócone\",\n",
    "    # W realizacji\n",
    "    \"w realizacji\": \"w_realizacji\",\n",
    "    \"W Realizacji\": \"w_realizacji\",\n",
    "    \"W trakcie realizacji\": \"w_realizacji\",\n",
    "    \"w_realizacji\": \"w_realizacji\",\n",
    "    # Oczekuje na płatność\n",
    "    \"oczekuje\": \"oczekuje_platnosc\",\n",
    "    \"oczekuje na płatność\": \"oczekuje_platnosc\",\n",
    "    \"Oczekuje na płatnosc\": \"oczekuje_platnosc\",\n",
    "    \"do zapłaty\": \"oczekuje_platnosc\",\n",
    "}\n",
    "\n",
    "# Zastosowanie\n",
    "kolumna_czysta = kolumna_statusow.map(MAPA_STATUSOW).fillna(\"nieznany\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Przed czyszczeniem\n",
    "top_przed = kolumna_statusow.value_counts().head(8)\n",
    "axes[0].barh(top_przed.index[::-1], top_przed.values[::-1],\n",
    "             color='#e74c3c', alpha=0.75)\n",
    "axes[0].set_title(f'Przed czyszczeniem\\n({kolumna_statusow.nunique()} unikalnych wartości)', fontsize=10)\n",
    "axes[0].set_xlabel('Liczba rekordów')\n",
    "\n",
    "# Po czyszczeniu\n",
    "po_czyszczeniu = kolumna_czysta.value_counts()\n",
    "kolory_po = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6', '#95a5a6']\n",
    "axes[1].bar(po_czyszczeniu.index, po_czyszczeniu.values,\n",
    "            color=kolory_po[:len(po_czyszczeniu)], alpha=0.85, edgecolor='white')\n",
    "axes[1].set_title(f'Po czyszczeniu (słownik AI)\\n({kolumna_czysta.nunique()} unikalnych wartości)', fontsize=10)\n",
    "axes[1].set_ylabel('Liczba rekordów')\n",
    "axes[1].tick_params(axis='x', rotation=20)\n",
    "\n",
    "plt.suptitle('Czyszczenie kategorii z pomocą AI — mapowanie wariantów', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nPrzed: {kolumna_statusow.nunique()} unikalnych wartości → Po: {kolumna_czysta.nunique()} unikalnych wartości\")\n",
    "print(f\"Pokrycie mapy: {(~kolumna_czysta.eq('nieznany')).mean()*100:.1f}% rekordów zmapowanych poprawnie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 8: Automatyczne executive summary\n",
    "\n",
    "Technika 4: Dane z analizy → prompt z szablonem → gotowy raport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 8: AUTOMATYCZNE EXECUTIVE SUMMARY ===\n",
    "\n",
    "# Zbieramy dane z poprzednich sekcji do słownika\n",
    "raport_kat = raport_kategorii(df_sprzedaz)\n",
    "\n",
    "przychod_total = raport_kat['przychod'].sum()\n",
    "top_kategoria = raport_kat.iloc[0]['produkt_kategoria']\n",
    "top_udzial = raport_kat.iloc[0]['udzial_pct']\n",
    "\n",
    "# Status breakdown\n",
    "status_counts = df_sprzedaz['status'].value_counts(normalize=True) * 100\n",
    "wskaznik_zwrotow = status_counts.get('zwrócone', 0)\n",
    "wskaznik_anulowanych = status_counts.get('anulowane', 0)\n",
    "\n",
    "dane_do_raportu = {\n",
    "    'przychod_total': przychod_total,\n",
    "    'top_kategoria': top_kategoria,\n",
    "    'top_udzial': top_udzial,\n",
    "    'wskaznik_zwrotow': wskaznik_zwrotow,\n",
    "    'wskaznik_anulowanych': wskaznik_anulowanych,\n",
    "    'ab_test_roznica': roznica,\n",
    "    'ab_test_p': p_val,\n",
    "}\n",
    "\n",
    "print(\"Dane do raportu:\")\n",
    "for k, v in dane_do_raportu.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MOCK AI: EXECUTIVE SUMMARY ===\n",
    "\n",
    "def mock_executive_summary(dane: dict) -> str:\n",
    "    \"\"\"Symuluje wygenerowanie executive summary przez model AI.\"\"\"\n",
    "    p = dane['przychod_total']\n",
    "    top = dane['top_kategoria']\n",
    "    top_u = dane['top_udzial']\n",
    "    zwr = dane['wskaznik_zwrotow']\n",
    "    anu = dane['wskaznik_anulowanych']\n",
    "    ab_r = dane['ab_test_roznica']\n",
    "    ab_p = dane['ab_test_p']\n",
    "\n",
    "    ab_wniosek = (\n",
    "        f\"potwierdzono statystycznie (p={ab_p:.4f}) — wzrost o +{ab_r:.0f} PLN/transakcję\"\n",
    "        if ab_p < 0.05\n",
    "        else f\"brak statystycznego potwierdzenia (p={ab_p:.4f})\"\n",
    "    )\n",
    "\n",
    "    zwrot_ocena = (\n",
    "        \"wymaga uwagi (powyżej normy 10%)\" if zwr > 10\n",
    "        else \"w normie (poniżej 10%)\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"**Executive Summary — Analiza zamówień e-commerce**\n",
    "*(wygenerowane automatycznie przez AI na podstawie analizy danych)*\n",
    "\n",
    "---\n",
    "\n",
    "**Wyniki ogólne:**\n",
    "Łączny przychód z zamówień zrealizowanych wyniósł {p:,.0f} PLN.\n",
    "Dominującą kategorią jest {top} z udziałem {top_u}% w przychodach.\n",
    "\n",
    "**Wskaźniki operacyjne:**\n",
    "- Wskaźnik zwrotów: {zwr:.1f}% — {zwrot_ocena}\n",
    "- Wskaźnik anulowanych: {anu:.1f}%\n",
    "- Kampania e-mail (A/B test): efekt personalizacji {ab_wniosek}\n",
    "\n",
    "**Trzy rekomendacje:**\n",
    "1. Zbadać przyczyny zwrotów w kategorii {top} — wskaźnik {zwr:.1f}% jest znaczący\n",
    "2. Wdrożyć personalizację e-mail jako standard (potwierdzona efektywność)\n",
    "3. Zwiększyć udział kategorii z wyższą marżą (sport, dom_i_ogrod) kosztem niżej marżowych\n",
    "\n",
    "---\n",
    "*Uwaga: Executive summary wygenerowany przez AI — wymaga weryfikacji merytorycznej przez analityka.*\"\"\"\n",
    "\n",
    "\n",
    "summary = mock_executive_summary(dane_do_raportu)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sekcja 9: Ograniczenia AI — ważne dla analityka\n",
    "\n",
    "AI jest potężnym narzędziem, ale ma konkretne ograniczenia których musisz być świadomy/a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 9: DEMONSTRACJA OGRANICZEŃ ===\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OGRANICZENIA LLM — KONKRETNE PRZYKŁADY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. HALUCYNACJE\n",
    "   Pytanie: 'Jaka funkcja pandas zwraca korelację?'\n",
    "\n",
    "   Odpowiedź poprawna:  df.corr() lub df['a'].corr(df['b'])\n",
    "   Możliwa halucynacja: df.correlation() <-- NIE ISTNIEJE\n",
    "                        df.pearson_corr() <-- NIE ISTNIEJE\n",
    "\n",
    "   Reguła: ZAWSZE uruchamiaj wygenerowany kod.\n",
    "           Błąd: AttributeError = halucynacja funkcji\n",
    "\n",
    "2. KNOWLEDGE CUTOFF\n",
    "   Pytanie: 'Jakie są najlepsze biblioteki do AI agentów w 2025?'\n",
    "\n",
    "   Model z cutoff 2024: nie zna nowości z 2025\n",
    "   Może polecać przestarzałe biblioteki lub wersje API\n",
    "\n",
    "   Reguła: dla aktualnych bibliotek → sprawdź oficjalną dokumentację\n",
    "\n",
    "3. MATEMATYKA\n",
    "   LLM = model językowy, NIE kalkulator\n",
    "   Może popełniać błędy arytmetyczne\n",
    "\n",
    "   Przykład: GPT-4 czasem myli 12% z 0.12 w obliczeniach wielokrokowych\n",
    "\n",
    "   Reguła: obliczenia zawsze wykonaj Pythonem, nie ufaj liczbie z AI\n",
    "\n",
    "4. PRYWATNOŚĆ DANYCH\n",
    "   Dane wysłane do publicznego API:\n",
    "   - Mogą być logowane przez dostawcę\n",
    "   - Mogą być użyte do trenowania (zależy od ustawień)\n",
    "   - Mogą być przechowywane poza UE (RODO!)\n",
    "\n",
    "   Reguła: dane osobowe klientów → lokalne modele lub zgoda prawników\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PODSUMOWANIE SEKCJI ===\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PODSUMOWANIE NOTEBOOK'U W14\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "OMÓWIONE TECHNIKI:\n",
    "\n",
    "1. Tokenizacja — LLM widzi tokeny, nie znaki\n",
    "   Języki fleksyjne (PL) = więcej tokenów = wyższy koszt\n",
    "\n",
    "2. Temperatura\n",
    "   KOD → temperatura 0.0-0.2\n",
    "   RAPORTY → temperatura 0.3-0.7\n",
    "   BRAINSTORMING → temperatura 1.0-1.5\n",
    "\n",
    "3. Modele: GPT (OpenAI), Claude (Anthropic), Gemini (Google)\n",
    "   Różne mocne strony — podobna struktura API\n",
    "\n",
    "4. Generowanie kodu: KONTEKST + ZADANIE + FORMAT + OGRANICZENIA\n",
    "   → Zawsze uruchamiaj i weryfikuj wygenerowany kod\n",
    "\n",
    "5. Interpretacja wyników: wyniki numeryczne + prompt → tekst dla zarządu\n",
    "\n",
    "6. Czyszczenie danych: AI generuje słownik mapowania w sekundy\n",
    "   → Ręcznie sprawdzasz mapowanie, nie tworzysz go od zera\n",
    "\n",
    "7. Executive summary: template + dane = raport\n",
    "\n",
    "NARZĘDZIA DLA STUDENTA (DARMOWE):\n",
    "   ✓ Claude.ai — free tier (~20 wiad/kilka h)\n",
    "   ✓ ChatGPT — free tier (GPT-4o mini)\n",
    "   ✓ GitHub Copilot — darmowy dla studentów (GitHub Education)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
